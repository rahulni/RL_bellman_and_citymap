{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# NumPy is used for numerical operations and easy reshaping of arrays\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Grid and Environment Setup\n",
    "# -------------------------------\n",
    "\n",
    "N = 4                          # Grid size: 4x4\n",
    "num_states = N * N             # Total number of states = 16\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Hyperparameters\n",
    "# -------------------------------\n",
    "\n",
    "gamma = 1.0                    # Discount factor (no discounting)\n",
    "theta = 1e-4                   # Convergence threshold\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Initialize Value Function\n",
    "# -------------------------------\n",
    "\n",
    "# V[s] represents the value of state s\n",
    "# Initialize all state values to 0 as per the problem statement\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Terminal State Definition\n",
    "# -------------------------------\n",
    "\n",
    "# Bottom-right corner of the grid is the terminal state\n",
    "# State indexing is row-major, so last index = 15\n",
    "terminal_state = num_states - 1\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Action Space\n",
    "# -------------------------------\n",
    "\n",
    "# Actions represented as (row change, column change)\n",
    "# Up, Down, Left, Right\n",
    "actions = [\n",
    "    (-1, 0),   # Up\n",
    "    (1, 0),    # Down\n",
    "    (0, -1),   # Left\n",
    "    (0, 1)     # Right\n",
    "]\n",
    "\n",
    "# Each action is taken with equal probability (0.25)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Helper Functions\n",
    "# -------------------------------\n",
    "\n",
    "def state_to_pos(s):\n",
    "    \"\"\"\n",
    "    Convert a 1D state index to a 2D grid position (row, col)\n",
    "    Example: state 6 -> (1, 2)\n",
    "    \"\"\"\n",
    "    return divmod(s, N)\n",
    "\n",
    "def pos_to_state(r, c):\n",
    "    \"\"\"\n",
    "    Convert a 2D grid position (row, col) back to a 1D state index\n",
    "    Example: (1, 2) -> 6\n",
    "    \"\"\"\n",
    "    return r * N + c\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Value Iteration Loop\n",
    "# -------------------------------\n",
    "\n",
    "while True:\n",
    "    delta = 0                  # Tracks maximum change in value function\n",
    "    V_new = V.copy()           # Copy old values to compute updates safely\n",
    "\n",
    "    # Iterate over all states\n",
    "    for s in range(num_states):\n",
    "\n",
    "        # Skip terminal state (value remains 0)\n",
    "        if s == terminal_state:\n",
    "            continue\n",
    "\n",
    "        # Convert state index to grid coordinates\n",
    "        r, c = state_to_pos(s)\n",
    "\n",
    "        value = 0              # Accumulate expected value for state s\n",
    "\n",
    "        # -------------------------------\n",
    "        # 8. Bellman Expectation Update\n",
    "        # -------------------------------\n",
    "\n",
    "        # For each possible action\n",
    "        for dr, dc in actions:\n",
    "\n",
    "            # Compute tentative next position\n",
    "            nr, nc = r + dr, c + dc\n",
    "\n",
    "            # Boundary handling:\n",
    "            # If action takes agent outside the grid,\n",
    "            # agent stays in the same state\n",
    "            if nr < 0 or nr >= N or nc < 0 or nc >= N:\n",
    "                s_next = s\n",
    "            else:\n",
    "                s_next = pos_to_state(nr, nc)\n",
    "\n",
    "            # Reward is -1 for every move\n",
    "            reward = -1\n",
    "\n",
    "            # Bellman expectation equation:\n",
    "            # V(s) += 0.25 * [reward + gamma * V(s')]\n",
    "            value += 0.25 * (reward + gamma * V[s_next])\n",
    "\n",
    "        # Store updated value for state s\n",
    "        V_new[s] = value\n",
    "\n",
    "        # Track the maximum change across states\n",
    "        delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "    # Update value function after full sweep\n",
    "    V = V_new\n",
    "\n",
    "    # -------------------------------\n",
    "    # 9. Convergence Check\n",
    "    # -------------------------------\n",
    "\n",
    "    # Stop when maximum change is below threshold\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Display Final Value Function\n",
    "# -------------------------------\n",
    "\n",
    "# Reshape the 1D value array into a 4x4 grid for readability\n",
    "V_grid = V.reshape(N, N)\n",
    "print(V_grid)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}